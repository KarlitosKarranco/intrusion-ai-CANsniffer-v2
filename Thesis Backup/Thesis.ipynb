{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe7b7vhEQTQb",
        "outputId": "a24065e1-0a97-45c9-9049-9c2c91a1a5a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.0`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFH5IGr3cY1f",
        "outputId": "0809b1e8-6623-4704-8776-1a1a8674bfb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.3.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwy7OypscjKJ",
        "outputId": "126c4d40-8bf5-4c9d-d324-39edb7f259d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.10.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 39.2MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.18.5)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.32.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.35.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0) (2.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (50.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (4.6)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0) (3.2.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=261ba8ed7a9ea7dc60daad8a9b5ae28bf301d0acbfd1ce2d14f3fd5fd77912f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, keras-applications, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym2hjlJnQWCG",
        "outputId": "2cf124f1-a8be-41d6-b370-eade5fcecd3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import itertools as it\n",
        "import tensorflow as tf\n",
        "import sklearn as skl\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Input\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "# Sliding window length that segments the data\n",
        "SLIDING_WINDOW_LENGTH = 60\n",
        "\n",
        "# Batch Size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Number of filters in the convolutional layers\n",
        "NUM_FILTERS = 64\n",
        "\n",
        "# Size of the filters in the convolutional layers\n",
        "FILTER_SIZE = 1\n",
        "\n",
        "# Number of unit in the long short-term recurrent layers\n",
        "NUM_UNITS_LSTM = 120\n",
        "\n",
        "# Number of units in the self attention mechanism\n",
        "ATTENTION_UNITS = 60\n",
        "\n",
        "# The local width of the attention mechanism\n",
        "ATTENTION_WIDTH = 10\n",
        "\n",
        "# Number of epochs to train our model\n",
        "EPOCHS = 20\n",
        "\n",
        "# Const number that defines the training learning rate, or how quickly the models converges to a solution\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# The number of epochs until the learning rate is exponentially decreased\n",
        "LR_EPOCHS = 7\n",
        "\n",
        "# The seed that is used to set the random number generator\n",
        "SEED = 0\n",
        "\n",
        "# The const dataset and directory paths\n",
        "BASE_PATH = '/content/drive/My Drive/intrusion-ai-CANsniffer-v2/'\n",
        "PASSIVE_DATA_PATH = BASE_PATH + os.path.join('Dataset', 'Processed', 'Passive', 'passive-dataset.csv')\n",
        "NPY_PASSIVE_DATA_PATH = BASE_PATH + os.path.join('Dataset', 'Processed', 'Passive', 'passive-dataset.npy')\n",
        "AGGRESSIVE_DATA_PATH = BASE_PATH + os.path.join('Dataset', 'Processed', 'Aggressive', 'aggressive-dataset.csv')\n",
        "NPY_AGGRESSIVE_DATA_PATH = BASE_PATH + os.path.join('Dataset', 'Processed', 'Aggressive', 'aggressive-dataset.npy')\n",
        "MODEL_SAVE_DIR = BASE_PATH + os.path.join('Saved_Models')\n",
        "\n",
        "\n",
        "class SeqSelfAttention(keras.layers.Layer):\n",
        "    ATTENTION_TYPE_ADD = 'additive'\n",
        "    ATTENTION_TYPE_MUL = 'multiplicative'\n",
        "    def __init__(self,\n",
        "                 units=32,\n",
        "                 attention_width=None,\n",
        "                 attention_type=ATTENTION_TYPE_ADD,\n",
        "                 return_attention=False,\n",
        "                 history_only=False,\n",
        "                 kernel_initializer='glorot_normal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 use_additive_bias=True,\n",
        "                 use_attention_bias=True,\n",
        "                 attention_activation=None,\n",
        "                 attention_regularizer_weight=0.0,\n",
        "                 **kwargs):\n",
        "        \"\"\"Layer initialization.\n",
        "        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n",
        "        :param units: The dimension of the vectors that used to calculate the attention weights.\n",
        "        :param attention_width: The width of local attention.\n",
        "        :param attention_type: 'additive' or 'multiplicative'.\n",
        "        :param return_attention: Whether to return the attention weights for visualization.\n",
        "        :param history_only: Only use historical pieces of data.\n",
        "        :param kernel_initializer: The initializer for weight matrices.\n",
        "        :param bias_initializer: The initializer for biases.\n",
        "        :param kernel_regularizer: The regularization for weight matrices.\n",
        "        :param bias_regularizer: The regularization for biases.\n",
        "        :param kernel_constraint: The constraint for weight matrices.\n",
        "        :param bias_constraint: The constraint for biases.\n",
        "        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n",
        "                                  in additive mode.\n",
        "        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n",
        "        :param attention_activation: The activation used for calculating the weights of attention.\n",
        "        :param attention_regularizer_weight: The weights of attention regularizer.\n",
        "        :param kwargs: Parameters for parent class.\n",
        "        \"\"\"\n",
        "        super(SeqSelfAttention, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.units = units\n",
        "        self.attention_width = attention_width\n",
        "        self.attention_type = attention_type\n",
        "        self.return_attention = return_attention\n",
        "        self.history_only = history_only\n",
        "        if history_only and attention_width is None:\n",
        "            self.attention_width = int(1e9)\n",
        "\n",
        "        self.use_additive_bias = use_additive_bias\n",
        "        self.use_attention_bias = use_attention_bias\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
        "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
        "        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n",
        "        self.bias_constraint = keras.constraints.get(bias_constraint)\n",
        "        self.attention_activation = keras.activations.get(attention_activation)\n",
        "        self.attention_regularizer_weight = attention_regularizer_weight\n",
        "        self._backend = keras.backend.backend()\n",
        "\n",
        "        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
        "            self.Wx, self.Wt, self.bh = None, None, None\n",
        "            self.Wa, self.ba = None, None\n",
        "        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
        "            self.Wa, self.ba = None, None\n",
        "        else:\n",
        "            raise NotImplementedError('No implementation for attention type : ' + attention_type)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.units,\n",
        "            'attention_width': self.attention_width,\n",
        "            'attention_type': self.attention_type,\n",
        "            'return_attention': self.return_attention,\n",
        "            'history_only': self.history_only,\n",
        "            'use_additive_bias': self.use_additive_bias,\n",
        "            'use_attention_bias': self.use_attention_bias,\n",
        "            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),\n",
        "            'bias_initializer': keras.initializers.serialize(self.bias_initializer),\n",
        "            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),\n",
        "            'bias_regularizer': keras.regularizers.serialize(self.bias_regularizer),\n",
        "            'kernel_constraint': keras.constraints.serialize(self.kernel_constraint),\n",
        "            'bias_constraint': keras.constraints.serialize(self.bias_constraint),\n",
        "            'attention_activation': keras.activations.serialize(self.attention_activation),\n",
        "            'attention_regularizer_weight': self.attention_regularizer_weight,\n",
        "        }\n",
        "        base_config = super(SeqSelfAttention, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
        "            self._build_additive_attention(input_shape)\n",
        "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
        "            self._build_multiplicative_attention(input_shape)\n",
        "        super(SeqSelfAttention, self).build(input_shape)\n",
        "\n",
        "    def _build_additive_attention(self, input_shape):\n",
        "        feature_dim = int(input_shape[2])\n",
        "\n",
        "        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n",
        "                                  name='{}_Add_Wt'.format(self.name),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n",
        "                                  name='{}_Add_Wx'.format(self.name),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "        if self.use_additive_bias:\n",
        "            self.bh = self.add_weight(shape=(self.units,),\n",
        "                                      name='{}_Add_bh'.format(self.name),\n",
        "                                      initializer=self.bias_initializer,\n",
        "                                      regularizer=self.bias_regularizer,\n",
        "                                      constraint=self.bias_constraint)\n",
        "\n",
        "        self.Wa = self.add_weight(shape=(self.units, 1),\n",
        "                                  name='{}_Add_Wa'.format(self.name),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "        if self.use_attention_bias:\n",
        "            self.ba = self.add_weight(shape=(1,),\n",
        "                                      name='{}_Add_ba'.format(self.name),\n",
        "                                      initializer=self.bias_initializer,\n",
        "                                      regularizer=self.bias_regularizer,\n",
        "                                      constraint=self.bias_constraint)\n",
        "\n",
        "    def _build_multiplicative_attention(self, input_shape):\n",
        "        feature_dim = int(input_shape[2])\n",
        "\n",
        "        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n",
        "                                  name='{}_Mul_Wa'.format(self.name),\n",
        "                                  initializer=self.kernel_initializer,\n",
        "                                  regularizer=self.kernel_regularizer,\n",
        "                                  constraint=self.kernel_constraint)\n",
        "        if self.use_attention_bias:\n",
        "            self.ba = self.add_weight(shape=(1,),\n",
        "                                      name='{}_Mul_ba'.format(self.name),\n",
        "                                      initializer=self.bias_initializer,\n",
        "                                      regularizer=self.bias_regularizer,\n",
        "                                      constraint=self.bias_constraint)\n",
        "\n",
        "    def call(self, inputs, mask=None, **kwargs):\n",
        "        input_len = K.shape(inputs)[1]\n",
        "\n",
        "        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n",
        "            e = self._call_additive_emission(inputs)\n",
        "        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n",
        "            e = self._call_multiplicative_emission(inputs)\n",
        "\n",
        "        if self.attention_activation is not None:\n",
        "            e = self.attention_activation(e)\n",
        "        if self.attention_width is not None:\n",
        "            if self.history_only:\n",
        "                lower = K.arange(0, input_len) - (self.attention_width - 1)\n",
        "            else:\n",
        "                lower = K.arange(0, input_len) - self.attention_width // 2\n",
        "            lower = K.expand_dims(lower, axis=-1)\n",
        "            upper = lower + self.attention_width\n",
        "            indices = K.expand_dims(K.arange(0, input_len), axis=0)\n",
        "            e -= 10000.0 * (1.0 - K.cast(lower <= indices, K.floatx()) * K.cast(indices < upper, K.floatx()))\n",
        "        if mask is not None:\n",
        "            mask = K.expand_dims(K.cast(mask, K.floatx()), axis=-1)\n",
        "            e -= 10000.0 * ((1.0 - mask) * (1.0 - K.permute_dimensions(mask, (0, 2, 1))))\n",
        "\n",
        "        # a_{t} = \\text{softmax}(e_t)\n",
        "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
        "        a = e / K.sum(e, axis=-1, keepdims=True)\n",
        "\n",
        "        # l_t = \\sum_{t'} a_{t, t'} x_{t'}\n",
        "        v = K.batch_dot(a, inputs)\n",
        "        if self.attention_regularizer_weight > 0.0:\n",
        "            self.add_loss(self._attention_regularizer(a))\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [v, a]\n",
        "        return v\n",
        "\n",
        "    def _call_additive_emission(self, inputs):\n",
        "        input_shape = K.shape(inputs)\n",
        "        batch_size, input_len = input_shape[0], input_shape[1]\n",
        "\n",
        "        # h_{t, t'} = \\tanh(x_t^T W_t + x_{t'}^T W_x + b_h)\n",
        "        q = K.expand_dims(K.dot(inputs, self.Wt), 2)\n",
        "        k = K.expand_dims(K.dot(inputs, self.Wx), 1)\n",
        "        if self.use_additive_bias:\n",
        "            h = K.tanh(q + k + self.bh)\n",
        "        else:\n",
        "            h = K.tanh(q + k)\n",
        "\n",
        "        # e_{t, t'} = W_a h_{t, t'} + b_a\n",
        "        if self.use_attention_bias:\n",
        "            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n",
        "        else:\n",
        "            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n",
        "        return e\n",
        "\n",
        "    def _call_multiplicative_emission(self, inputs):\n",
        "        # e_{t, t'} = x_t^T W_a x_{t'} + b_a\n",
        "        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n",
        "        if self.use_attention_bias:\n",
        "            e += self.ba[0]\n",
        "        return e\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = input_shape\n",
        "        if self.return_attention:\n",
        "            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n",
        "            return [output_shape, attention_shape]\n",
        "        return output_shape\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        if self.return_attention:\n",
        "            return [mask, None]\n",
        "        return mask\n",
        "\n",
        "    def _attention_regularizer(self, attention):\n",
        "        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n",
        "        input_len = K.shape(attention)[-1]\n",
        "        indices = K.expand_dims(K.arange(0, input_len), axis=0)\n",
        "        diagonal = K.expand_dims(K.arange(0, input_len), axis=-1)\n",
        "        eye = K.cast(K.equal(indices, diagonal), K.floatx())\n",
        "        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n",
        "            attention,\n",
        "            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def get_custom_objects():\n",
        "        return {'SeqSelfAttention': SeqSelfAttention}\n",
        "\n",
        "\n",
        "def deep_conv_lstm_self_attention():\n",
        "    \"\"\"\n",
        "    This function defines a DeepConvLSTM with a Self Attention mechanism model.\n",
        "    :return: DeepConvLSTM-SelfAttention model.\n",
        "    \"\"\"\n",
        "    # Input shape is num of time steps * features\n",
        "    input_layer = Input(shape=(SLIDING_WINDOW_LENGTH, 10), batch_size=BATCH_SIZE)\n",
        "    conv = Conv1D(NUM_FILTERS, FILTER_SIZE, padding='same', activation='relu', input_shape=(SLIDING_WINDOW_LENGTH, 10))(input_layer)\n",
        "    conv2 = Conv1D(NUM_FILTERS, FILTER_SIZE, padding='same', activation='relu')(conv)\n",
        "    conv3 = Conv1D(NUM_FILTERS, FILTER_SIZE, padding='same', activation='relu')(conv2)\n",
        "    lstm = LSTM(NUM_UNITS_LSTM, dropout=0.3, activation='tanh', return_sequences=True)(conv3)\n",
        "    attention = SeqSelfAttention(ATTENTION_UNITS, attention_width=ATTENTION_WIDTH, attention_activation='tanh')\n",
        "    attention_out = attention(lstm)\n",
        "    dense = Dense(1024, activation='relu')(attention_out)\n",
        "    output_layer = Dense(1, activation='sigmoid')(dense)\n",
        "    model_ = Model(inputs=input_layer, outputs=output_layer)\n",
        "    print(model_.summary())\n",
        "    return model_\n",
        "\n",
        "\n",
        "def learning_scheduler(epoch):\n",
        "    \"\"\"\n",
        "    This function keeps the learning rate at [LEARNING_RATE] for the first [LR_EPOCHS] epochs\n",
        "    and decreases it exponentially after that.\n",
        "    :return: The learning rate.\n",
        "    \"\"\"\n",
        "    if epoch <= LR_EPOCHS:\n",
        "        lr = LEARNING_RATE\n",
        "        print('LR IS:', lr)\n",
        "        return float(lr)\n",
        "    else:\n",
        "        lr = LEARNING_RATE * np.exp(0.1 * (LR_EPOCHS - epoch))\n",
        "        print('LR IS:', lr)\n",
        "        return float(lr)\n",
        "\n",
        "\n",
        "def sliding_window(data, length, step=1):\n",
        "    \"\"\"\n",
        "    This function splits our processed dataset into windows of size [SLIDING_WINDOW_LENGTH].\n",
        "    :return: NumPy array that contains the windowed dataset.\n",
        "    \"\"\"\n",
        "    streams = it.tee(data, length)\n",
        "    a = zip(*[it.islice(stream, i, None, step*length) for stream, i in zip(streams, it.count(step=step))])\n",
        "    b = list(a)\n",
        "    return np.asarray(b)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\nTensorflow version: \", tf.__version__)\n",
        "    print(\"Eager mode: \", tf.executing_eagerly())\n",
        "    print(\"GPU is\", \"available!\" if tf.config.experimental.list_physical_devices('GPU') else \"not available!\\n\")\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    # Check if model save directory exists, else mkdir\n",
        "    if not os.path.exists(os.path.join(MODEL_SAVE_DIR)):\n",
        "        os.mkdir(os.path.join(MODEL_SAVE_DIR))\n",
        "\n",
        "    # Set random seeds for libraries\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(SEED)\n",
        "\n",
        "    passive = []\n",
        "    aggressive = []\n",
        "\n",
        "    # Check if passive numpy saves exist, if so load .npy, else read in .csv\n",
        "    if os.path.exists(os.path.join(NPY_PASSIVE_DATA_PATH)):\n",
        "        print('Reading in saved passive NumPy dataset...')\n",
        "        passive = np.load(NPY_PASSIVE_DATA_PATH)\n",
        "    else:\n",
        "        print('No saved passive dataset, reading in and processing new data.')\n",
        "        passive = np.genfromtxt(PASSIVE_DATA_PATH, delimiter=',', skip_header=1, dtype=int)\n",
        "        np.save(NPY_PASSIVE_DATA_PATH, passive)\n",
        "\n",
        "    # Check if aggressive numpy saves exist, if so load .npy, else read in .csv\n",
        "    if os.path.exists(os.path.join(NPY_AGGRESSIVE_DATA_PATH)):\n",
        "        print('Reading in saved aggressive NumPy dataset...')\n",
        "        aggressive = np.load(NPY_AGGRESSIVE_DATA_PATH)\n",
        "    else:\n",
        "        print('No saved aggressive dataset, reading in and processing new data.')\n",
        "        aggressive = np.genfromtxt(AGGRESSIVE_DATA_PATH, delimiter=',', skip_header=1, dtype=int)\n",
        "        np.save(NPY_AGGRESSIVE_DATA_PATH, aggressive)\n",
        "\n",
        "    # Extract labels\n",
        "    passive_labels = passive[:, -1]\n",
        "    aggressive_labels = aggressive[:, -1]\n",
        "\n",
        "    # Generate sliding windows on passive and aggressive\n",
        "    passive_ = sliding_window(passive[:, :-1], SLIDING_WINDOW_LENGTH)\n",
        "    passive_labels_ = sliding_window(passive_labels, SLIDING_WINDOW_LENGTH)\n",
        "    aggressive_ = sliding_window(aggressive[:, :-1], SLIDING_WINDOW_LENGTH)\n",
        "    aggressive_labels_ = sliding_window(aggressive_labels, SLIDING_WINDOW_LENGTH)\n",
        "\n",
        "    # Concatenate and then shuffle the dataset windows around\n",
        "    X = np.concatenate((passive_, aggressive_))\n",
        "    y = np.concatenate((passive_labels_, aggressive_labels_))\n",
        "    X_, y_ = skl.utils.shuffle(X, y, random_state=SEED)\n",
        "\n",
        "    # Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.2, random_state=SEED)\n",
        "\n",
        "    # Prompt the user to supply 'y' or 'n' to retrain model\n",
        "    retrainCheck = input('Do you want to train a new model (y/n)? ')\n",
        "    if retrainCheck == 'y':\n",
        "        # Initialise our DeepConvLSTM with Self-Attention Mechanism model\n",
        "        model = deep_conv_lstm_self_attention()\n",
        "\n",
        "        # Sets our callback for the learning rate scheduler\n",
        "        lrs = LearningRateScheduler(learning_scheduler)\n",
        "\n",
        "        # Sets our callback for the early stopper\n",
        "        es = EarlyStopping(monitor='loss', min_delta=0.005, patience=1, verbose=0, mode='min',\n",
        "                           restore_best_weights=False, baseline=None)\n",
        "\n",
        "        # Print the shapes and number of training instances and testing instances\n",
        "        print('Number of training instances:', X_train.shape[0])\n",
        "        print('Number of test instances:    ', X_test.shape[0])\n",
        "\n",
        "        # X_train, y_train, X_test, and y_test are all numpy arrays\n",
        "        print('X_train.shape =', X_train.shape)\n",
        "        print('y_train.shape =', y_train.shape)\n",
        "        print('X_test.shape =', X_test.shape)\n",
        "        print('y_test.shape =', y_test.shape)\n",
        "\n",
        "        # Fit the model to the training data\n",
        "        optimizer = tf.keras.optimizers.Adam()\n",
        "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.05, callbacks=[lrs, es])\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred_score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "        # Let's take a look at our model's loss and accuracy score\n",
        "        print(\"DeepConvLSTM-Self-Attention accuracy: \", y_pred_score[1])\n",
        "        print(\"DeepConvLSTM-Self-Attention loss: \", y_pred_score[0])\n",
        "\n",
        "        # Check to see if we have a previous best model saved\n",
        "        if os.path.exists(os.path.join(MODEL_SAVE_DIR, 'best_model.h5')):\n",
        "            # Load our previous best model\n",
        "            savedModel = load_model(os.path.join(MODEL_SAVE_DIR, 'best_model.h5'), \n",
        "                                    custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
        "\n",
        "            # Evaluate our old model\n",
        "            try:\n",
        "              savedModelScore = savedModel.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "            except:\n",
        "              print('The previously saved model expects different sliding window lengths.')\n",
        "              print('Please remove saved model if you wish to train on a different sliding window length.')\n",
        "              quit()\n",
        "\n",
        "            # If it is better (based on accuracy), then we should overwrite our previous best model\n",
        "            if y_pred_score[1] > savedModelScore[1]:\n",
        "                model.save(os.path.join(MODEL_SAVE_DIR, 'best_model.h5'))\n",
        "                print('New best model saved!')\n",
        "        # If no previous best model, then save this new model as our best\n",
        "        else:\n",
        "            model.save(os.path.join(MODEL_SAVE_DIR, 'best_model.h5'))\n",
        "            print('Saved model!')\n",
        "    elif retrainCheck == 'n':\n",
        "        # Check to see if we have a previous best model saved\n",
        "        if os.path.exists(os.path.join(MODEL_SAVE_DIR, 'best_model.h5')):\n",
        "            # Load in our saved model, custom_objects must be defined to load it correctly\n",
        "            model = load_model(os.path.join(MODEL_SAVE_DIR, 'best_model.h5'),\n",
        "                               custom_objects={'SeqSelfAttention': SeqSelfAttention})\n",
        "            print(model.summary())\n",
        "\n",
        "            # Evaluate the model\n",
        "            y_pred_score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "            # Let's take a look at our model's loss and accuracy score\n",
        "            print(\"DeepConvLSTM-Self-Attention accuracy: \", y_pred_score[1])\n",
        "            print(\"DeepConvLSTM-Self-Attention loss: \", y_pred_score[0])\n",
        "        # Prompt user that there is no previously saved model.\n",
        "        else:\n",
        "            print('No previous model has been saved. Please train a new model.')\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tensorflow version:  2.0.0\n",
            "Eager mode:  True\n",
            "GPU is available!\n",
            "Reading in saved passive NumPy dataset...\n",
            "Reading in saved aggressive NumPy dataset...\n",
            "Do you want to train a new model (y/n)? n\n",
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        [(128, 60, 10)]           0         \n",
            "_________________________________________________________________\n",
            "conv1d_51 (Conv1D)           (128, 60, 64)             704       \n",
            "_________________________________________________________________\n",
            "conv1d_52 (Conv1D)           (128, 60, 64)             4160      \n",
            "_________________________________________________________________\n",
            "conv1d_53 (Conv1D)           (128, 60, 64)             4160      \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (128, 60, 256)            328704    \n",
            "_________________________________________________________________\n",
            "seq_self_attention_17 (SeqSe (128, 60, 256)            65793     \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (128, 60, 1024)           263168    \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (128, 60, 1)              1025      \n",
            "=================================================================\n",
            "Total params: 667,714\n",
            "Trainable params: 667,714\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "DeepConvLSTM-Self-Attention accuracy:  0.98080605\n",
            "DeepConvLSTM-Self-Attention loss:  0.0473744648911412\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}