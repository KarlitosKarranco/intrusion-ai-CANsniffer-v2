{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DeepConvLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwmoojJWE2so",
        "outputId": "bce51d86-09d6-4968-a1be-ccc653a9a720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount raw gdrive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KEuFYn_ECUO",
        "outputId": "8d4704e9-d52e-4331-c6da-da0a50f55d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.stats as st\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, SimpleRNN, Dropout, Conv2D, Lambda, Input, Bidirectional, Flatten\n",
        "from tensorflow.keras.layers import Layer, Dense, Flatten, Activation, Permute\n",
        "from tensorflow.keras.layers import Multiply, Lambda, Reshape, Dot, Concatenate, RepeatVector, TimeDistributed, Permute, Bidirectional\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.classification import accuracy_score, recall_score, f1_score\n",
        "\n",
        "# ImportError: cannot import name 'CuDNNLSTM' site:stackoverflow.com"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZFAp4hG71E"
      },
      "source": [
        "\"\"\"\n",
        "Created on Wed Jun 19 20:08:11 2019\n",
        "\n",
        "@author: ongunuzaymacar\n",
        "\n",
        "Script containing custom layer implementations for a family of attention mechanisms in TensorFlow\n",
        "with Keras integration (tested for TF 2.0). Comments next to each operation in each layer indicate\n",
        "the output shapes. For ease of notation, the following abbreviations are used:\n",
        "i)    B  = batch size,\n",
        "ii)   S  = sequence length (many-to-one) OR input sequence length (many-to-many),\n",
        "iii)  S' = target sequence length (many-to-many),\n",
        "iv)   S* = optimized (by 'local' approach, sometimes referred to as 'alignment length') sequence\n",
        "           length,\n",
        "v)    S- = the larger of the sequence lengths for many-to-many scenarios,\n",
        "vi)   V  = vocabulary size,\n",
        "vii)  H  = number of hidden dimensions,\n",
        "viii) E  = number of embedding dimensions\n",
        "\n",
        "Additionally, if a tensors shape differs for many-to-one and many-to-many scenarios, <1> and <M>\n",
        "tags will respectively identify the corresponding shapes. If no distinction is made, assume that\n",
        "the shape indicated is applicable for both scenarios.\n",
        "\"\"\"\n",
        "\n",
        "class Attention(Layer):\n",
        "    \"\"\"\n",
        "    Layer for implementing two common types of attention mechanisms, i) global (soft) attention\n",
        "    and ii) local (hard) attention, for two types of sequence tasks, i) many-to-one and\n",
        "    ii) many-to-many.\n",
        "\n",
        "    The setting use_bias=False converts the Dense() layers into annotation weight matrices. Softmax\n",
        "    activation ensures that all weights sum up to 1. Read more here to make more sense of the code\n",
        "    and implementations:\n",
        "    i)   https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention\n",
        "    ii)  https://github.com/philipperemy/keras-attention-mechanism/issues/14\n",
        "    iii) https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
        "\n",
        "    SUGGESTION: If model doesn't converge or the test accuracy is lower than expected, try playing\n",
        "    around with the hidden size of the recurrent layers, the batch size in training process, or the\n",
        "    param @window_width if using a 'local' attention.\n",
        "\n",
        "    NOTE: This implementation takes the hidden states associated with the last timestep of the input\n",
        "    sequence as the target hidden state (h_t) as suggested by @felixhao28 in i) for many-to-one\n",
        "    scenarios. Hence, when trying to predict what word (token) comes after sequence ['I', 'love',\n",
        "    'biscuits', 'and'], we take h('and') with shape (1, H) as the target hidden state. For\n",
        "    many-to-many scenarios, it takes the hidden state associated with the timestep that is being\n",
        "    currently iterated in the target sequence, usually by a decoder-like architecture.\n",
        "\n",
        "    @param (str) context: the context of the problem at hand, specify 'many-to-many' for\n",
        "           sequence-to-sequence tasks such as machine translation and question answering, or\n",
        "           specify 'many-to-one' for tasks such as sentiment classification and language modelling\n",
        "    @param (str) alignment_type: type of attention mechanism to be applied, 'local-m' corresponds to\n",
        "           monotonic alignment where we take the last @window_width timesteps, 'local-p' corresponds\n",
        "           to having a Gaussian distribution around the predicted aligned position, whereas\n",
        "           'local-p*' corresponds to the newly proposed method to adaptively learning the unique\n",
        "           timesteps to give attention (currently only works for many-to-one scenarios)\n",
        "    @param (int) window_width: width for set of source hidden states in 'local' attention\n",
        "    @param (str) score_function: alignment score function config; current implementations include\n",
        "           the 'dot', 'general', and 'location' both by Luong et al. (2015), 'concat' by Bahdanau et\n",
        "           al. (2015), and 'scaled_dot' by Vaswani et al. (2017)\n",
        "    @param (str) model_api: specify to use TF's Sequential OR Functional API, note that attention\n",
        "           weights are not outputted with the former as it only accepts single-output layers\n",
        "    \"\"\"\n",
        "    def __init__(self, context='many-to-many', alignment_type='global', window_width=None,\n",
        "                 score_function='general', model_api='functional', **kwargs):\n",
        "        if context not in ['many-to-many', 'many-to-one']:\n",
        "            raise ValueError(\"Argument for param @context is not recognized\")\n",
        "        if alignment_type not in ['global', 'local-m', 'local-p', 'local-p*']:\n",
        "            raise ValueError(\"Argument for param @alignment_type is not recognized\")\n",
        "        if alignment_type == 'global' and window_width is not None:\n",
        "            raise ValueError(\"Can't use windowed approach with global attention\")\n",
        "        if context == 'many-to-many' and alignment_type == 'local-p*':\n",
        "            raise ValueError(\"Can't use local-p* approach in many-to-many scenarios\")\n",
        "        if score_function not in ['dot', 'general', 'location', 'concat', 'scaled_dot']:\n",
        "            raise ValueError(\"Argument for param @score_function is not recognized\")\n",
        "        if model_api not in ['sequential', 'functional']:\n",
        "            raise ValueError(\"Argument for param @model_api is not recognized\")\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "        self.context = context\n",
        "        self.alignment_type = alignment_type\n",
        "        self.window_width = window_width  # D\n",
        "        self.score_function = score_function\n",
        "        self.model_api = model_api\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(Attention, self).get_config()\n",
        "        base_config['alignment_type'] = self.alignment_type\n",
        "        base_config['window_width'] = self.window_width\n",
        "        base_config['score_function'] = self.score_function\n",
        "        base_config['model_api'] = self.model_api\n",
        "        return base_config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Declare attributes for easy access to dimension values\n",
        "        if self.context == 'many-to-many':\n",
        "            self.input_sequence_length, self.hidden_dim = input_shape[0][1], input_shape[0][2]\n",
        "            self.target_sequence_length = input_shape[1][1]\n",
        "        elif self.context == 'many-to-one':\n",
        "            self.input_sequence_length, self.hidden_dim = input_shape[0][1], input_shape[0][2]\n",
        "\n",
        "        # Build weight matrices for different alignment types and score functions\n",
        "        if 'local-p' in self.alignment_type:\n",
        "            self.W_p = Dense(units=self.hidden_dim, use_bias=False)\n",
        "            self.W_p.build(input_shape=(None, None, self.hidden_dim))                               # (B, 1, H)\n",
        "            self._trainable_weights += self.W_p.trainable_weights\n",
        "\n",
        "            self.v_p = Dense(units=1, use_bias=False)\n",
        "            self.v_p.build(input_shape=(None, None, self.hidden_dim))                               # (B, 1, H)\n",
        "            self._trainable_weights += self.v_p.trainable_weights\n",
        "\n",
        "        if 'dot' not in self.score_function:  # weight matrix not utilized for 'dot' function\n",
        "            self.W_a = Dense(units=self.hidden_dim, use_bias=False)\n",
        "            self.W_a.build(input_shape=(None, None, self.hidden_dim))                               # (B, S*, H)\n",
        "            self._trainable_weights += self.W_a.trainable_weights\n",
        "\n",
        "        if self.score_function == 'concat':  # define additional weight matrices\n",
        "            self.U_a = Dense(units=self.hidden_dim, use_bias=False)\n",
        "            self.U_a.build(input_shape=(None, None, self.hidden_dim))                               # (B, 1, H)\n",
        "            self._trainable_weights += self.U_a.trainable_weights\n",
        "\n",
        "            self.v_a = Dense(units=1, use_bias=False)\n",
        "            self.v_a.build(input_shape=(None, None, self.hidden_dim))                               # (B, S*, H)\n",
        "            self._trainable_weights += self.v_a.trainable_weights\n",
        "\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Pass decoder output (prev. timestep) alongside encoder output for all scenarios\n",
        "        if not isinstance(inputs, list):\n",
        "            raise ValueError(\"Pass a list=[encoder_out (Tensor), decoder_out (Tensor),\" +\n",
        "                             \"current_timestep (int)] for all scenarios\")\n",
        "\n",
        "        # Specify source and target states (and timestep if applicable) for easy access\n",
        "        if self.context == 'many-to-one':\n",
        "            # Get h_t, the current (target) hidden state as the last timestep of input sequence\n",
        "            target_hidden_state = inputs[1]                                                         # (B, H)\n",
        "            source_hidden_states = inputs[0]                                                        # (B, S, H)\n",
        "        elif self.context == 'many-to-many':\n",
        "            # Get h_t, the current (target) hidden state from the previous decoded hidden state\n",
        "            target_hidden_state = inputs[1]                                                         # (B, H)\n",
        "            current_timestep = inputs[2]\n",
        "            source_hidden_states = inputs[0]                                                        # (B, S, H)\n",
        "\n",
        "        # Add time axis to h_t\n",
        "        target_hidden_state = tf.expand_dims(input=target_hidden_state, axis=1)                     # (B, 1, H)\n",
        "\n",
        "        # Get h_s, source hidden states through specified attention mechanism\n",
        "        if self.alignment_type == 'global':                                                         # Global Approach\n",
        "            source_hidden_states = source_hidden_states                                             # (B, S, H)\n",
        "\n",
        "        elif 'local' in self.alignment_type:                                                        # Local Approach\n",
        "            # Automatically set window width to default value (8 -> no real logic behind this value)\n",
        "            self.window_width = 8 if self.window_width is None else self.window_width\n",
        "\n",
        "            # Get aligned position (between inputs & targets) and derive a context window to focus\n",
        "            if self.alignment_type == 'local-m':                                                    # Monotonic Alignment\n",
        "                # Set alignment position\n",
        "                if self.context == 'many-to-one':\n",
        "                    aligned_position = self.input_sequence_length\n",
        "                elif self.context == 'many-to-many':\n",
        "                    aligned_position = current_timestep\n",
        "                # Get window borders\n",
        "                left = int(aligned_position - self.window_width\n",
        "                           if aligned_position - self.window_width >= 0\n",
        "                           else 0)\n",
        "                right = int(aligned_position + self.window_width\n",
        "                            if aligned_position + self.window_width <= self.input_sequence_length\n",
        "                            else self.input_sequence_length)\n",
        "                # Extract window window\n",
        "                source_hidden_states = Lambda(lambda x: x[:, left:right, :])(source_hidden_states)  # (B, S*=(D, 2xD), H)\n",
        "\n",
        "            elif self.alignment_type == 'local-p':                                                  # Predictive Alignment\n",
        "                aligned_position = self.W_p(target_hidden_state)                                    # (B, 1, H)\n",
        "                aligned_position = Activation('tanh')(aligned_position)                             # (B, 1, H)\n",
        "                aligned_position = self.v_p(aligned_position)                                       # (B, 1, 1)\n",
        "                aligned_position = Activation('sigmoid')(aligned_position)                          # (B, 1, 1)\n",
        "                aligned_position = aligned_position * self.input_sequence_length                    # (B, 1, 1)\n",
        "\n",
        "            elif self.alignment_type == 'local-p*':                                                 # Completely Predictive Alignment\n",
        "                aligned_position = self.W_p(source_hidden_states)                                   # (B, S, H)\n",
        "                aligned_position = Activation('tanh')(aligned_position)                             # (B, S, H)\n",
        "                aligned_position = self.v_p(aligned_position)                                       # (B, S, 1)\n",
        "                aligned_position = Activation('sigmoid')(aligned_position)                          # (B, S, 1)\n",
        "                # Only keep top D values out of the sigmoid activation, and zero-out the rest\n",
        "                aligned_position = tf.squeeze(aligned_position, axis=-1)                            # (B, S)\n",
        "                top_probabilities = tf.nn.top_k(input=aligned_position,                             # (values:(B, D), indices:(B, D))\n",
        "                                                k=self.window_width,\n",
        "                                                sorted=False)\n",
        "                onehot_vector = tf.one_hot(indices=top_probabilities.indices,\n",
        "                                           depth=self.input_sequence_length)                        # (B, D, S)\n",
        "                onehot_vector = tf.reduce_sum(onehot_vector, axis=1)                                # (B, S)\n",
        "                aligned_position = Multiply()([aligned_position, onehot_vector])                    # (B, S)\n",
        "                aligned_position = tf.expand_dims(aligned_position, axis=-1)                        # (B, S, 1)\n",
        "                initial_source_hidden_states = source_hidden_states                                 # (B, S, 1)\n",
        "                source_hidden_states = Multiply()([source_hidden_states, aligned_position])         # (B, S*=S(D), H)\n",
        "                # Scale back-to approximately original hidden state values\n",
        "                aligned_position += tf.keras.backend.epsilon()                                      # (B, S, 1)\n",
        "                source_hidden_states /= aligned_position                                            # (B, S*=S(D), H)\n",
        "                source_hidden_states = initial_source_hidden_states + source_hidden_states          # (B, S, H)\n",
        "\n",
        "        # Compute alignment score through specified function\n",
        "        if 'dot' in self.score_function:                                                            # Dot Score Function\n",
        "            attention_score = Dot(axes=[2, 2])([source_hidden_states, target_hidden_state])         # (B, S*, 1)\n",
        "            if self.score_function == 'scaled_dot':\n",
        "                attention_score *= 1 / np.sqrt(float(source_hidden_states.shape[2]))                # (B, S*, 1)\n",
        "\n",
        "        elif self.score_function == 'general':                                                      # General Score Function\n",
        "            weighted_hidden_states = self.W_a(source_hidden_states)                                 # (B, S*, H)\n",
        "            attention_score = Dot(axes=[2, 2])([weighted_hidden_states, target_hidden_state])       # (B, S*, 1)\n",
        "\n",
        "        elif self.score_function == 'location':                                                     # Location-based Score Function\n",
        "            weighted_target_state = self.W_a(target_hidden_state)                                   # (B, 1, H)\n",
        "            attention_score = Activation('softmax')(weighted_target_state)                          # (B, 1, H)\n",
        "            attention_score = RepeatVector(source_hidden_states.shape[1])(attention_score)          # (B, S*, H)\n",
        "            attention_score = tf.reduce_sum(attention_score, axis=-1)                               # (B, S*)\n",
        "            attention_score = tf.expand_dims(attention_score, axis=-1)                              # (B, S*, 1)\n",
        "\n",
        "        elif self.score_function == 'concat':                                                       # Concat Score Function\n",
        "            weighted_hidden_states = self.W_a(source_hidden_states)                                 # (B, S*, H)\n",
        "            weighted_target_state = self.U_a(target_hidden_state)                                   # (B, 1, H)\n",
        "            weighted_sum = weighted_hidden_states + weighted_target_state                           # (B, S*, H)\n",
        "            weighted_sum = Activation('tanh')(weighted_sum)                                         # (B, S*, H)\n",
        "            attention_score = self.v_a(weighted_sum)                                                # (B, S*, 1)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = Activation('softmax')(attention_score)                                  # (B, S*, 1)\n",
        "\n",
        "        # Distribute weights around aligned position for local-p approach only\n",
        "        if self.alignment_type == 'local-p':                                                        # Gaussian Distribution\n",
        "            gaussian_estimation = lambda s: tf.exp(-tf.square(s - aligned_position) /\n",
        "                                                   (2 * tf.square(self.window_width / 2)))\n",
        "            gaussian_factor = gaussian_estimation(0)\n",
        "            for i in range(1, self.input_sequence_length):\n",
        "                gaussian_factor = Concatenate(axis=1)([gaussian_factor, gaussian_estimation(i)])    # (B, S*, 1)\n",
        "            attention_weights = attention_weights * gaussian_factor                                 # (B, S*, 1)\n",
        "\n",
        "        # Derive context vector\n",
        "        context_vector = source_hidden_states * attention_weights                                   # (B, S*, H)\n",
        "\n",
        "        if self.model_api == 'functional':\n",
        "            return context_vector, attention_weights\n",
        "        elif self.model_api == 'sequential':\n",
        "            return context_vector\n",
        "\n",
        "\n",
        "class SelfAttention(Layer):\n",
        "    \"\"\"\n",
        "    Layer for implementing self-attention mechanism. Weight variables were preferred over Dense()\n",
        "    layers in implementation because they allow easier identification of shapes. Softmax activation\n",
        "    ensures that all weights sum up to 1.\n",
        "\n",
        "    @param (int) size: a.k.a attention length, number of hidden units to decode the attention before\n",
        "           the softmax activation and becoming annotation weights\n",
        "    @param (int) num_hops: number of hops of attention, or number of distinct components to be\n",
        "           extracted from each sentence.\n",
        "    @param (bool) use_penalization: set True to use penalization, otherwise set False\n",
        "    @param (int) penalty_coefficient: the weight of the extra loss\n",
        "    @param (str) model_api: specify to use TF's Sequential OR Functional API, note that attention\n",
        "           weights are not outputted with the former as it only accepts single-output layers\n",
        "    \"\"\"\n",
        "    def __init__(self, size, num_hops=8, use_penalization=True,\n",
        "                 penalty_coefficient=0.1, model_api='functional', batch_size = 1, **kwargs):\n",
        "        if model_api not in ['sequential', 'functional']:\n",
        "            raise ValueError(\"Argument for param @model_api is not recognized\")\n",
        "        self.size = size\n",
        "        self.num_hops = num_hops\n",
        "        self.use_penalization = use_penalization\n",
        "        self.penalty_coefficient = penalty_coefficient\n",
        "        self.model_api = model_api\n",
        "        self.batch_size = batch_size\n",
        "        super(SelfAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super(SelfAttention, self).get_config()\n",
        "        base_config['size'] = self.size\n",
        "        base_config['batch_size'] = self.batch_size\n",
        "        base_config['num_hops'] = self.num_hops\n",
        "        base_config['use_penalization'] = self.use_penalization\n",
        "        base_config['penalty_coefficient'] = self.penalty_coefficient\n",
        "        base_config['model_api'] = self.model_api\n",
        "        return base_config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W1 = self.add_weight(name='W1',\n",
        "                                  shape=(self.size, int(input_shape[2])),                                # (size, H)\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        self.W2 = self.add_weight(name='W2',\n",
        "                                  shape=(self.num_hops, self.size),                                 # (num_hops, size)\n",
        "                                  initializer='glorot_uniform',\n",
        "                                  trainable=True)\n",
        "        super(SelfAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):  # (B, S, H)\n",
        "        # Expand weights to include batch size through implicit broadcasting\n",
        "        W1, W2 = self.W1[None, :, :], self.W2[None, :, :]\n",
        "        W1, W2 = tf.tile(W1, [self.batch_size, 1, 1]), tf.tile(W2, [self.batch_size, 1, 1])\n",
        "        #W1, W2 = tf.compat.v1.repeat(W1, repeats = [self.batch_size], axis=0), tf.compat.v1.repeat(W2, repeats = [self.batch_size], axis=0)\n",
        "        hidden_states_transposed = Permute(dims=(2, 1))(inputs)                                     # (B, H, S)\n",
        "        attention_score = tf.matmul(W1, hidden_states_transposed)                                   # (B, size, S)\n",
        "        attention_score = Activation('tanh')(attention_score)                                       # (B, size, S)\n",
        "        attention_weights = tf.matmul(W2, attention_score)                                          # (B, num_hops, S)\n",
        "        attention_weights = Activation('softmax')(attention_weights)                                # (B, num_hops, S)\n",
        "        embedding_matrix = tf.matmul(attention_weights, inputs)                                     # (B, num_hops, H)\n",
        "        embedding_matrix_flattened = Flatten()(embedding_matrix)                                    # (B, num_hops*H)\n",
        "\n",
        "        if self.use_penalization:\n",
        "            attention_weights_transposed = Permute(dims=(2, 1))(attention_weights)                  # (B, S, num_hops)\n",
        "            product = tf.matmul(attention_weights, attention_weights_transposed)                    # (B, num_hops, num_hops)\n",
        "            identity = tf.eye(self.num_hops, batch_shape=(inputs.shape[0],))                        # (B, num_hops, num_hops)\n",
        "            frobenius_norm = tf.sqrt(tf.reduce_sum(tf.square(product - identity)))  # distance\n",
        "            self.add_loss(self.penalty_coefficient * frobenius_norm)  # loss\n",
        "\n",
        "        if self.model_api == 'functional':\n",
        "            return embedding_matrix_flattened, attention_weights\n",
        "        elif self.model_api == 'sequential':\n",
        "            return embedding_matrix_flattened\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IOXjsdGLfXv"
      },
      "source": [
        "def model(x_train, num_labels, LSTM_units, num_conv_filters, batch_size, F, D):\n",
        "    \"\"\"\n",
        "    The proposed model with CNN layer, LSTM RNN layer and self attention layers.\n",
        "    Inputs:\n",
        "    - x_train: required for creating input shape for RNN layer in Keras\n",
        "    - num_labels: number of output classes (int)\n",
        "    - LSTM_units: number of RNN units (int)\n",
        "    - num_conv_filters: number of CNN filters (int)\n",
        "    - batch_size: number of samples to be processed in each batch\n",
        "    - F: the attention length (int)\n",
        "    - D: the length of the output (int) \n",
        "    Returns\n",
        "    - model: A Keras model\n",
        "    \"\"\"\n",
        "    cnn_inputs = Input(shape=(x_train.shape[1], x_train.shape[2], 1), batch_size=batch_size, name='rnn_inputs')\n",
        "    cnn_layer = Conv2D(num_conv_filters, kernel_size = (1, x_train.shape[2]), strides=(1, 1), padding='valid', data_format=\"channels_last\")\n",
        "    cnn_out = cnn_layer(cnn_inputs)\n",
        "\n",
        "    sq_layer = Lambda(lambda x: K.squeeze(x, axis = 2))\n",
        "    sq_layer_out = sq_layer(cnn_out)\n",
        "\n",
        "    rnn_layer = LSTM(LSTM_units, return_sequences=True, name='lstm', return_state=True) #return_state=True\n",
        "    rnn_layer_output, _, _ = rnn_layer(sq_layer_out)\n",
        "\n",
        "    encoder_output, attention_weights = SelfAttention(size=F, num_hops=D, use_penalization=False, batch_size = batch_size)(rnn_layer_output)\n",
        "    dense_layer = Dense(num_labels, activation = 'softmax')\n",
        "    dense_layer_output = dense_layer(encoder_output)\n",
        "\n",
        "    model = Model(inputs=cnn_inputs, outputs=dense_layer_output)\n",
        "    print (model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "# K.set_session(sess)\n",
        "\n",
        "EPOCH = 10\n",
        "BATCH_SIZE = 16\n",
        "LSTM_UNITS = 32\n",
        "CNN_FILTERS = 3\n",
        "NUM_LSTM_LAYERS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "PATIENCE = 20\n",
        "SEED = 0\n",
        "F = 32\n",
        "D = 10\n",
        "DATA_FILES = ['/content/drive/My Drive/intrusion-ai-CANsniffer-v2/Dataset/Test/data/LOTO/WISDM.npz']\n",
        "MODE = 'LOTO'\n",
        "BASE_DIR = '/content/drive/My Drive/intrusion-ai-CANsniffer-v2/Dataset/Test/data/' + MODE + '/'\n",
        "SAVE_DIR = '/content/drive/My Drive/intrusion-ai-CANsniffer-v2/Dataset/Test/'\n",
        "\n",
        "# if not os.path.exists(os.path.join(SAVE_DIR)):\n",
        "#     os.mkdir(os.path.join(SAVE_DIR))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    SEED = 0 \n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    tf.random.set_seed(0)\n",
        "\n",
        "    for DATA_FILE in DATA_FILES:\n",
        "        data_input_file = os.path.join(BASE_DIR, DATA_FILE)\n",
        "        tmp = np.load(data_input_file, allow_pickle=True)\n",
        "        X = tmp['X']\n",
        "        X = np.squeeze(X, axis = 1)\n",
        "        y_one_hot = tmp['y']\n",
        "        folds = tmp['folds']\n",
        "\n",
        "        NUM_LABELS = y_one_hot.shape[1]\n",
        "\n",
        "        avg_acc = []\n",
        "        avg_recall = []\n",
        "        avg_f1 = []\n",
        "        early_stopping_epoch_list = []\n",
        "        y = np.argmax(y_one_hot, axis=1)\n",
        "\n",
        "        for i in range(0, len(folds)):\n",
        "            train_idx = folds[i][0]\n",
        "            test_idx = folds[i][1]\n",
        "\n",
        "            X_train, y_train, y_train_one_hot = X[train_idx], y[train_idx], y_one_hot[train_idx]\n",
        "            X_test, y_test, y_test_one_hot = X[test_idx], y[test_idx], y_one_hot[test_idx]\n",
        "\n",
        "            X_train_ = np.expand_dims(X_train, axis = 3)\n",
        "            X_test_ = np.expand_dims(X_test, axis = 3)\n",
        "\n",
        "            train_trailing_samples =  X_train_.shape[0]%BATCH_SIZE\n",
        "            test_trailing_samples =  X_test_.shape[0]%BATCH_SIZE\n",
        "\n",
        "\n",
        "            if train_trailing_samples!= 0:\n",
        "                X_train_ = X_train_[0:-train_trailing_samples]\n",
        "                y_train_one_hot = y_train_one_hot[0:-train_trailing_samples]\n",
        "                y_train = y_train[0:-train_trailing_samples]\n",
        "            if test_trailing_samples!= 0:\n",
        "                X_test_ = X_test_[0:-test_trailing_samples]\n",
        "                y_test_one_hot = y_test_one_hot[0:-test_trailing_samples]\n",
        "                y_test = y_test[0:-test_trailing_samples]\n",
        "\n",
        "            print (y_train.shape, y_test.shape)   \n",
        "\n",
        "            rnn_model = model(x_train = X_train_, num_labels = NUM_LABELS, LSTM_units = LSTM_UNITS, \\\n",
        "                num_conv_filters = CNN_FILTERS, batch_size = BATCH_SIZE, F = F, D= D)\n",
        "\n",
        "            model_filename = SAVE_DIR + '/best_model_with_self_attn_' + str(DATA_FILE[0:-4]) + '_fold_' + str(i) + '.h5'\n",
        "            callbacks = [ModelCheckpoint(filepath=model_filename, monitor = 'val_acc', save_weights_only=True, save_best_only=True), EarlyStopping(monitor='val_acc', patience=PATIENCE)]#, LearningRateScheduler()]\n",
        "\n",
        "            opt = optimizers.Adam(clipnorm=1.)\n",
        "\n",
        "            rnn_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            history = rnn_model.fit(X_train_, y_train_one_hot, epochs=EPOCH, batch_size=BATCH_SIZE, verbose=1, callbacks=callbacks, validation_data=(X_test_, y_test_one_hot))\n",
        "\n",
        "            early_stopping_epoch = callbacks[1].stopped_epoch - PATIENCE + 1\n",
        "            print('Early stopping epoch: ' + str(early_stopping_epoch))\n",
        "            early_stopping_epoch_list.append(early_stopping_epoch)\n",
        "\n",
        "            if early_stopping_epoch <= 0:\n",
        "                early_stopping_epoch = -100\n",
        "\n",
        "            # Evaluate model and predict data on TEST \n",
        "            print(\"******Evaluating TEST set*********\")\n",
        "            rnn_model.load_weights(model_filename)\n",
        "\n",
        "            y_test_predict = rnn_model.predict(X_test_, batch_size = BATCH_SIZE)\n",
        "            y_test_predict = np.array(y_test_predict)\n",
        "            y_test_predict = np.argmax(y_test_predict, axis=1)\n",
        "\n",
        "            all_trainable_count = int(np.sum([K.count_params(p) for p in set(rnn_model.trainable_weights)]))\n",
        "\n",
        "            MAE = metrics.mean_absolute_error(y_test, y_test_predict, sample_weight=None, multioutput='uniform_average')\n",
        "\n",
        "            acc_fold = accuracy_score(y_test, y_test_predict)\n",
        "            avg_acc.append(acc_fold)\n",
        "\n",
        "            recall_fold = recall_score(y_test, y_test_predict, average='macro')\n",
        "            avg_recall.append(recall_fold)\n",
        "\n",
        "            f1_fold  = f1_score(y_test, y_test_predict, average='macro')\n",
        "            avg_f1.append(f1_fold)\n",
        "\n",
        "            with open(SAVE_DIR + '/results_model_with_self_attn_' + MODE + '.csv', 'a') as out_stream:\n",
        "                out_stream.write(str(SEED) + ', ' + str(DATA_FILE[0:-4]) + ', ' + str(i) + ', ' + str(early_stopping_epoch) + ', ' + str(all_trainable_count) + ', ' + str(acc_fold) + ', ' + str(MAE) + ', ' + str(recall_fold) + ', ' + str(f1_fold) + '\\n')\n",
        "\n",
        "\n",
        "            print('Accuracy[{:.4f}] Recall[{:.4f}] F1[{:.4f}] at fold[{}]'.format(acc_fold, recall_fold, f1_fold, i))\n",
        "            print('______________________________________________________')\n",
        "            K.clear_session()\n",
        "\n",
        "    ic_acc = st.t.interval(0.9, len(avg_acc) - 1, loc=np.mean(avg_acc), scale=st.sem(avg_acc))\n",
        "    ic_recall = st.t.interval(0.9, len(avg_recall) - 1, loc=np.mean(avg_recall), scale=st.sem(avg_recall))\n",
        "    ic_f1 = st.t.interval(0.9, len(avg_f1) - 1, loc = np.mean(avg_f1), scale=st.sem(avg_f1))\n",
        "\n",
        "    print('Mean Accuracy[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_acc), ic_acc[0], ic_acc[1]))\n",
        "    print('Mean Recall[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_recall), ic_recall[0], ic_recall[1]))\n",
        "    print('Mean F1[{:.4f}] IC [{:.4f}, {:.4f}]'.format(np.mean(avg_f1), ic_f1[0], ic_f1[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}